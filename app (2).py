# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IyYKSuiCaTPwt8tjAV2oCOrU_vcpJAb-
"""
# app.py
import streamlit as st
import torch
from transformers import (
    T5TokenizerFast, T5ForConditionalGeneration,
    DistilBertTokenizerFast, DistilBertForSequenceClassification,
    BertTokenizerFast, BertForSequenceClassification
)

# 1) Detectar dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
st.sidebar.write(f"**Dispositivo:** {device}")

# 2) Cargar y cachear modelos/tokenizers
@st.cache_resource(show_spinner=False)
def load_models():
    models = {}

    # T5 seq2seq
    t5_dir = "t5-fakenews"
    t5_tok = T5TokenizerFast.from_pretrained(t5_dir, local_files_only=True)
    t5_mod = T5ForConditionalGeneration.from_pretrained(t5_dir, local_files_only=True).to(device)
    # Pre-calcula IDs de tokens de salida
    fake_id = t5_tok.convert_tokens_to_ids("fake")
    real_id = t5_tok.convert_tokens_to_ids("real")
    models["T5"] = (t5_tok, t5_mod, fake_id, real_id)

    # DistilBERT
    db_dir = "Distilbert-fakenews"
    db_tok = DistilBertTokenizerFast.from_pretrained(db_dir, local_files_only=True)
    db_mod = DistilBertForSequenceClassification.from_pretrained(db_dir, local_files_only=True).to(device)
    models["DistilBERT"] = (db_tok, db_mod)

    # BERT
    bert_dir = "bert-fakenews"
    bert_tok = BertTokenizerFast.from_pretrained("bert-base-uncased")  # usa vocab oficial
    bert_mod = BertForSequenceClassification.from_pretrained(bert_dir, local_files_only=True).to(device)
    models["BERT"] = (bert_tok, bert_mod)

    return models

models = load_models()

# 3) Funciones de inferencia
def predict_t5(text):
    tok, mod, fake_id, real_id = models["T5"]
    inputs = tok("classify: " + text, return_tensors="pt", truncation=True, max_length=256).to(device)
    with torch.no_grad():
        logits = mod(**inputs).logits  # [1, seq, vocab]
    probs = torch.softmax(logits[:, 0, :], dim=-1)[0]
    p_fake, p_real = probs[fake_id].item(), probs[real_id].item()
    label = "fake" if p_fake > p_real else "real"
    return label, {"fake": p_fake, "real": p_real}

def predict_cls(text, key):
    tok, mod = models[key]
    inputs = tok(text, return_tensors="pt", truncation=True,
                 padding="max_length", max_length=256).to(device)
    with torch.no_grad():
        logits = mod(**inputs).logits
    probs = torch.softmax(logits, dim=-1)[0]
    # id2label mapping
    id2lab = mod.config.id2label
    # build dict
    conf = {id2lab[i]: probs[i].item() for i in range(len(probs))}
    # take top
    label = max(conf, key=conf.get)
    return label, conf

# 4) Streamlit UI
st.title("üì∞ Fake News Detection")

st.markdown(
    """
    Ingrese un texto en el √°rea de abajo y seleccione el modelo para clasificar
    como **real** o **falso**, junto con sus puntajes de confianza.
    """
)

model = st.selectbox("Seleccionar Modelo", ["T5", "DistilBERT", "BERT"])
text  = st.text_area("Texto de la noticia:", height=200)

if st.button("Predecir"):
    if not text.strip():
        st.warning("Escribe algo de texto primero.")
    else:
        st.info(f"üîç Analizando con **{model}**...")
        if model == "T5":
            label, conf = predict_t5(text)
        else:
            label, conf = predict_cls(text, model)
        st.subheader(f"Predicci√≥n: **{label}**")
        st.write("Confianzas:")
        st.json(conf)
